大数据框架（处理海量离线数据/处理实时流式数据）
	一：以hadoop2.x版本为体系的海量数据处理框架
		离线数据分析，往往分析的式N+1的数据
		- Mapreduce
			并行计算，分而治之
			- HDFS（分布式数据的存储）
			- Yarn（分布式资源管理和任务调度）
			缺点：
				主要依赖磁盘，对磁盘依赖太高（io）
				shuflle，map讲数据写入到本次磁盘，reduce通过网络的方式将map task任务产生的数据拉取到reduce端
							
		- Hive 数据仓库的工具（从事数据挖掘方向，一定要掌握hive，hive的优化、调优）
			底层还是调用mr
			impala（性能非常高，基于内存的）
		
		- Sqoop
			桥梁：RDBMS（关系型数据库）-》HDFS/Hive
				  HDFS/Hive	-> RDBMS（关系型数据库)
		
		- HABSE	
			列式Nosql存储数据库，大数据的分布式数据库
		
		- Hadoop如果做机器学习的话需要使用mahout库
		
	二：以storm为体系的实时流式处理框架
		JStorm（Java编写，阿里巴巴）
		实时数据分析-》进行实时分析
		场景：股票交易、金融、电商、导航系统
		
	
	三：以spark为体系的大数据框架
		基于内存的
			将数据的中间结果写入到内存（内存+磁盘）
		
		核心编程：
			Spark Core：RDD（弹性分布式数据集），类似于Mr
			Spark Sql：类似于Hive
			Spark Streaming: Strom
				结构化流
			
		高级编程：
			机器学习、深度学习、人工智能
			Spark MLLib: 机器学习（推荐系统）
			Spark Graphx: 图计算（广告系统）
			
	四：Flink
		社区不够活跃，阿里已经放弃了JStorm，转而使用Flink基础开发的BLink
		阿里、美团、滴滴
		

Spark 学习计划
第一部分：scala编程语言
第二部分：Spark Core（最重要的内容）-》核心概念RDD
第三部分：Spark Sql（相当于Hive， 支持两种操作语句：SQL和DSL）-》底层依赖的还是RDD
第四部分：Spark Streaming：主要做实时数据分析，相当于Storm
	注意：Spark Streaming实时性比较差


学习建议：
	- 类比法
		Scala：Java
		Spark Core：Mr
		Spark Sql：Hive
		Spark Streaming：Core，基于CoreApi的高度封装
	
	- 听、练、记、理解
		- 听：认真听
		- 练：多练，自己敲代码，严禁拷贝
		- 记：做笔记
		- 思：思考、理解
		
======================================第一部分：Scala编程语言=============================================================================
一：scala语言基础
	*）scala简介
		1）多范式编程语言
			面向对象编程+面向函数式编程
			架构师阅读你的源码（review）
		2）编程简单
			代码量大概式10：1的关系
		3）scala的语言诞生了两个大数据框架：
			- spark
				分布式海量数据处理框架
			- kafka
				分布式流平台，基于消息的发布和订阅框架，存储数据
			
		4）学习网站
			官网：
				https://www.scala-lang.org/
			推特：
				http://twitter.github.io/scala_school/zh_cn/
				
		5）版本选择
			2.11.x版本， 推荐2.11.12
			
			备注说明：
				- spark 1.6版本			推荐的是scala 2.10.x版本（很多企业目前还是采用该版本，这个版本非常稳定）
				- spark 2.x版本			推荐的scala 2.11.x版本
				
			
	*）scala的安装
		参考讲义
		
	
	*）写个scala程序
	
	*）常用的开发工具
		参考讲义
	
	*）变量的声明和定义
		有两种方式声明变量
		var（vriable） 和 val（value）
		var：可变的
		val：不可变的
		
		标准格式：val name:String = "tom"
			val：变量声明
			name：变量名
			：分隔符
			String：变量类型
			“tom”: 变量值
			
		不标准格式：
			 val name = "tom"
			 
			scala> val name:String = _
			<console>:11: error: unbound placeholder parameter
				   val name:String = _
									 ^
			scala> var name:String = _
			name: String = null

		注意：如果说变量的值是默认值的话，那么变量的声明一定是需要可变的，否则就没有声明变量的实际意义了
		
		String类型的默认值是null， int类型的默认值是0， Double的默认值是0.0
		
		一定注意：在Spark中能使用val的话，绝不使用var
		如果不显式的指定变量的类型，那么会根据变量的值进行自动的类型推倒
		
	*）scala数据类型
		常用的数据类型
			Byte：8位的有符号的		-128-127
			Char：
			Short：16位有符号		-32768-32767
			Int：  32位有符号	
			Long： 64位有符号	
			Float：浮点数
			Double：双精度
			Bolean
			
		跟java的区别
			在scala中，任何数据都是对象
				举例：数字1-》也是对象，因为它有方法
				
		在scala中存在很多增强数据类型的类，增加了对象的方法
	
		在scala中不存在自增、自减、三目运算符

	*）字符串插值操作
		scala> var src = "hello ${name}"
		src: String = hello ${name}

		scala> var name = "tom"
		name: String = tom

		scala> var src = s"hello ${name}"
		src: String = hello tom

	*）	Nothing表示在程序运行时产生的Exception
			scala> def f = throw new Exception("error")
			f: Nothing
			
		Null、Nil、Nothing分别代表什么意思
	
	*）scala的条件表达式
		有三种表达式的结构
		
		IF(boolean)  {}
		
		IF(boolean) {} else {}
		
		IF(i=1) {} else IF (i==2) {} else {}
	
		最大的区别：可以有返回值，并且可以用变量接收
		这时候返回的数据类型是Any，Any是所有数据类型的基类
		
	*）循环
		For 循环
			循环表达式
			在java中循环
			for(int i=0; i<=10;i++)
			
		While
		
		Do	。。。	While
		
		foeach
	
		需要注意的是：不能使用break跳出循环
		
	
二：scala面向函数式编程（最有特色的一部分）
	*）如何定义个方法
		标准的方法定义格式：
			def m(x:Int, y:Int):Unit = x*y
			
			def：定义方法使用def作为关键字
			m: 方法名
			(x:Int, y:Int)：参数列表
			Unit：返回值类型，Unit表示无返回值，类似于Java的void
			x*y：方法体
			
			指定返回值是Int类型
			def m(x:Int, y:Int):Int = x*y
			
			忽略返回值类型，根据方法体的最后一句话进行自动的推导
			def m(x:Int, y:Int) = x*y
			
			如果方法中没有参数的话。那么小括号是可以省略的，但是调用的时候，不能加括号了
			def m = println("hello world!")
			
		
	*）如何定义一个函数	
		两种定义方式
		第一种：
			scala> val f = (x:Int, y:Int) => x*y
			f: (Int, Int) => Int = <function2>

			val：定义函数关键字
			f：函数名
			(Int, Int)：参数类型
			Int：返回值类型
			<function2>：函数参数的个数，最多只能由22个参数，如果想使用更多参数的话，使用变长参数
			
			匿名函数，不指明函数名称
			(x:Int, y:Int) => x*y
			
			无参函数
			scala> () => 2
			res12: () => Int = <function0>

		第二种
			//调用函数的时候一定要加(), 不能省略，省略的话，会打印函数签名
			scala> val f2:(Int, Int) => Int = ((x, y) => x*y)
			f2: (Int, Int) => Int = <function2>
			f2：函数名称
			(Int, Int)：函数参数类型
			Int：返回值类型
			(x, y)：参数列表
			x*y：函数体
			
			//完整写法
			scala> val f3:(Int, Int) => Int = (x:Int, y:Int) => x*y 
			f3: (Int, Int) => Int = <function2>
		
	*）方法和函数的区别
		将方法转换成函数，采用神奇的下划线
		scala> def m(x:Int, y:Int) = x*y
		m: (x: Int, y: Int)Int

		scala> val m2 = m(2, 3)
		m2: Int = 6
		
		scala> m2 _					//下划线前面一定要加个空格
		res17: () => Int = <function0>

		
	*）函数的求值策略
		函数有两种求值策略
		1）call by value：
			对函数的实参求值， 且只求一次
			
		2）call by name：
			函数的实参在函数体内部每次用到的时候，都会被求值
		
	 
	*）高阶函数
		
	*）闭包
		就是函数的嵌套，即：在一个函数定义中，包含另外一个函数的定义；并且在内函数中可以访问外函数中的变量。
		

三：scala的集合
			不可变长数组					可变长数组
			Array							ArrayBuffer
			List							ListBuffer
			
	如果使用可变长数组（集合）的话，需要显式的导入包：import scala.collection.mutable._
	
	*）数组
		可变数组和不可变数组
		不可变长数组：表示长度不可变，但是角标元素可变
		可变长数组：表示长度和角标元素都可变
		在scala中默认的数组都是不可变数组
		
		数组里面可以放任意类型，如果类型不一致，则返回Any类型
		scala> val arr = Array(1,2,3,4,5)
		arr: Array[Int] = Array(1, 2, 3, 4, 5)

		scala> val arr = Array("a",2,3,4,5)
		arr: Array[Any] = Array(a, 2, 3, 4, 5)

		scala> val arr = Array("a","b","c")
		arr: Array[String] = Array(a, b, c)
		
		显式指定了数据类型，则不可放其他数据类型
		scala> val arr = Array[Int]("a",2,3,4,5)
		<console>:11: error: type mismatch;
		 found   : String("a")
		 required: Int
			   val arr = Array[Int]("a",2,3,4,5)

		初始化5个元素的数组，必须加new关键字
		scala> val arr = new Array[Int](5)
		arr: Array[Int] = Array(0, 0, 0, 0, 0)
		
		scala> val arr = new Array[String](5)
		arr: Array[String] = Array(null, null, null, null, null)

	*）集合
		（序列）
		scala>  val list01 = List(4,5,6)
		list01: List[Int] = List(4, 5, 6)

		scala> list01.::(1,2,3)
		res23: List[Any] = List((1,2,3), 4, 5, 6)
		
		scala> list01.+:(1,2,3)
		res24: List[Any] = List((1,2,3), 4, 5, 6)

		scala> (1,2,3)::list01
		res25: List[Any] = List((1,2,3), 4, 5, 6)
		
		scala> (1,2,3)+:list01
		res26: List[Any] = List((1,2,3), 4, 5, 6)

		scala> List(1,2,3) ++ list01
		res28: List[Int] = List(1, 2, 3, 4, 5, 6)
	*）Map
		参考代码
	
	*）元祖
		最多可以有22个参数，如果只有2个参数，称为对偶元组，元祖的角标从1开始	
		
	*）Set
		无序的不可重复的
		
	*）集合的常用方法
		1）reduce
			scala> val arr = Array(1,2,5,7,10)
			arr: Array[Int] = Array(1, 2, 5, 7, 10)

			scala> arr.reduce
			def reduce[A1 >: Int](op: (A1, A1) => A1): A1
			
			scala> arr.reduce((x, y) => x+y)
			res24: Int = 25
			
			scala> arr.reduce((x, y) => x*y)
			res25: Int = 700

			reduce默认请求的是reduceLeft，从做到右开始计算
			scala> arr.reduce((x, y) => x-y)
			res26: Int = -23
			
			
			举例：
			scala> val arr = Array(1,2,3,4,5)
			arr: Array[Int] = Array(1, 2, 3, 4, 5)

			scala> arr.reduce(_-_)
			res27: Int = -13

			scala> arr.reduceRight(_-_)
			res28: Int = 3

			参考画图			
			
		2）fold方法
			中文折叠，叠加的意思，
			reduce是聚合的意思，从本质上说，fold函数是将一种格式的输入数据转换成另外一种格式的数据返回 
			
			scala> val list = List(1,2,3,4,5)
			list: List[Int] = List(1, 2, 3, 4, 5)

			scala> list.reduce(_+_)
			res33: Int = 15
			
			0是表示初始值，第一个下划线：是初始值或者临时计算的结果
			scala> list.fold(0)(_+_)
			res34: Int = 15

			初始值100，只初始化一次
			scala> list.fold(100)(_+_)
			res35: Int = 115
			
			步骤解析
			(((((100+1)+2)+3)+4)+5)
			......
			
			fold默认调用foldLeft(从左到右计算)
			foldRight（从右到左计算）

			默认值是字符串类型，List是数字集合的时候，不能使用fold，只能使用foldLeft或者foldRight
			scala> list.fold("hello")(_+_)
			<console>:16: error: type mismatch;
			 found   : Any
			 required: String
				   list.fold("hello")(_+_)
										^

			scala> list.foldLeft("hello")(_+_)
			res37: String = hello12345

			从右到左进行计算
			scala> list.foldRight("hello")(_+_)
			res38: String = 12345hello

			字符串和数字进行数学运算时则报错，但是+除外，因为+号可以表示字符串的拼接转换
			scala> list.foldLeft("hello")(_-_)
			<console>:16: error: value - is not a member of String
				   list.foldLeft("hello")(_-_)
										   ^
		3）sortBy（排序仅仅是改变了数据的顺序，而无法改变数据的类型或者值）
			scala> val list = List(1,2,4,-7,12,90)
			list: List[Int] = List(1, 2, 4, -7, 12, 90)

			scala> list.sortBy(x=> x)
			res40: List[Int] = List(-7, 1, 2, 4, 12, 90)
			
			-号之前一定有个空格
			scala> list.sortBy(x=> -x)
			res41: List[Int] = List(90, 12, 4, 2, 1, -7)
		
		4）sorted
			scala> list.sorted
			res43: List[Int] = List(-7, 1, 2, 4, 12, 90)

			scala> list.sorted.reverse
			res44: List[Int] = List(90, 12, 4, 2, 1, -7)

		5）sortwith（两两比较，类似于冒泡排序）
			scala> val list = List(1,2,4,-7,12,90)
			scala> list.sortWith
			   def sortWith(lt: (Int, Int) => Boolean): List[Int]

			scala> list.sortWith((x, y)=> x>y)
			res45: List[Int] = List(90, 12, 4, 2, 1, -7)

			scala> list.sortWith((x, y)=> x<y)
			res46: List[Int] = List(-7, 1, 2, 4, 12, 90)
		
		6）flatten（压平）
			scala> val list = List(List(1,2,3), List(4,5,6))
			list: List[List[Int]] = List(List(1, 2, 3), List(4, 5, 6))

			scala> list.flatten
			res48: List[Int] = List(1, 2, 3, 4, 5, 6)
			
		7）flatMap=flatten+Map
			scala> var lines = Array("spark hadoop hive", "spark hadoop hive", "hadoop")
			lines: Array[String] = Array(spark hadoop hive, spark hadoop hive, hadoop)

			第一种写法：
			scala> lines.flatMap(line => line.split(" ")).map(x=>(x, 1)).groupBy(x=>x._1).map(x=>(x._1, x._2.length))
			res53: scala.collection.immutable.Map[String,Int] = Map(hadoop -> 3, spark -> 2, hive -> 2)

			第二种写法：
			scala> lines.flatMap(line => line.split(" ")).map(x=>(x, 1)).groupBy(x=>x._1).map(x=>(x._1, x._2.map(_._2).sum))
			res55: scala.collection.immutable.Map[String,Int] = Map(hadoop -> 3, spark -> 2, hive -> 2)

			第三种写法：
			scala> lines.flatMap(line => line.split(" ")).map(x=>(x, 1)).groupBy(x=>x._1).map(x=>(x._1, x._2.foldLeft(0)(_+_._2)))
			res58: scala.collection.immutable.Map[String,Int] = Map(hadoop -> 3, spark -> 2, hive -> 2)
		
			第四种写法：
			scala> lines.flatMap(line => line.split(" ")).map(x=>(x, 1)).groupBy(x=>x._1).mapValues(_.foldLeft(0)(_+_._2))
			res59: scala.collection.immutable.Map[String,Int] = Map(hadoop -> 3, spark -> 2, hive -> 2)

			
			scala> val grouped = lines.flatMap(line => line.split(" ")).map(x=>(x, 1)).groupBy(x=>x._1)
			res57: scala.collection.immutable.Map[String,Array[(String, Int)]] = Map(
				hadoop -> Array((hadoop,1), (hadoop,1), (hadoop,1)), 
				spark -> Array((spark,1), (spark,1)), hive -> Array((hive,1), (hive,1))
			)
			//hadoop -> Array((hadoop,1), (hadoop,1), (hadoop,1))
			grouped.map(_)
			//Array((hadoop,1), (hadoop,1), (hadoop,1))
			grouped.mapValues(_.foldLeft(0)(_ + _._2))
			
		8）aggregate
			聚合
			scala> val list = List(1,2,3,4,5)
			list: List[Int] = List(1, 2, 3, 4, 5)
			
			第二个函数参数，不起作用，对于单机环境来说不起作用
			第一个参数，表示局部操作
			第二个参数，表示全局操作
			全局操作只是针对于分布式环境有效
			scala> list.aggregate(0)((x, y)=>x+y,(x, y)=>x+y)
			res101: Int = 15

			scala> list.aggregate(0)((x, y)=>x+y,(x, y)=>x*y)
			res102: Int = 15

			scala> list.aggregate(0)((x, y)=>x+y,(x, y)=>x-y)
			res103: Int = 15

			scala> list.aggregate(0)((x, y)=>x+y,(x, y)=>x-y)

四：scala面向对象编程
	*）面向对象的基本特征
		定义：把数据和操作数据的方法放到一起，作为一个整体就是类（class）
		面向对象编程的特征
			1）封装
			2）继承
			3）多态
		scala即是面向对象编程也是面向函数式编程-》多范式
	
	*）构造器
		类似于java中的构造方法
		1）主构造器
			在scala中，主构造器是和类名放在一起的，有且只有一个，与java不同，java可以有多个构造方法，多个构造方法之间可以实现重载
			在类中，没有定义在任何方法中的代码（包括成员类型），都属于主构造器的代码，且执行顺序与代码的书写的顺序是一致的，其实与java一样
			在java中方法之外的代码，在构造器调用之前最先执行，可以将这些代码看作也是一个主构造器中进行执行的
			主构造器还可以通过使用默认参数，来给参数默认的值
			
		2）辅助构造器
			辅助构造器可以有多个
			多个辅助构造器之间可以调用
			辅助构造器中的参数不可以使用val/var
			
		3）私有构造器
			私有构造器是针对于主构造器
			如果声明了私有构造器，那么只能被他的伴生对象访问
			如果主构造器设置为私有构造器，那么辅助构造器依然可以访问
		
		
	*）作用域
		1）默认是public
		2）跟java语言类似
		3）一般情况下
			public：表示公共的，任意都可以访问
			private：表示私有的， 表示当前类的对象都可以访问（伴生对象和伴生类依然可以访问）
			private[this]：当前对象（实例）都可以访问，其他对象不能访问,连伴生对象都不能访问
			private[package]：表示某个包下面可以访问，比如private[Spark]，表示在Spark包下所有的类都可以访问
			 
	*）伴生对象
		1）如果有一个class，还有一个与clas同名的object，那么就称这个object是class的伴生对象，class是object的伴生类
		2）伴生类和伴生对象必须放在一个.scala的文件中
		3）伴生类和伴生对象，最大的特点就在于，相互之间可以访问private的字段
		
		
	*）scala中的object
		1）object：相当于java的静态类，类似于java的单例模式，通常在里面放一些class里面共享的内容
		2）可以将object看成一个类class，只是这个类在内存中是一个单例，且定义的object就是实例名，不需要我们自己进行实例化，运行与JVM在java中帮我们new出来了
		3）第一次调用object方法时，会执行object的构造器，也就是说object内部不再方法中的代码（并且被执行一次），但是object不能定义接收参数的构造器
		4）注意object的构造器只是会在第一次调用的时候执行，以后每次调用不会再次执行构造器了
		
		使用场景：
			初始化配置文件
			公共处理类（方法）
			数据库连接初始化
			。。。。。
			
		
	*）scala中的apply方法
		1）当不是new关键字来创建对象的时候，使用apply可以是我们的代码更加简洁
		
	
	*）继承
		1）在scala中，让子类继承父类，与java一样，使用extends关键字
		2）继承就代表，子类可以从父类继承父类的field和method，然后子类可以在自己内部放入父类所没有，子类所特有的field和medthod
		3）子类可以覆盖父类的field和method，但是需要注意的时final关键字，filnal关键字表示field和method不能覆盖
		4）子类中的方法要覆盖父类的非抽象方法的话，需要override关键字
		5）子类的属性val要覆盖父类的非抽象属性，必须要override
		6）子类实现父类的抽象方法或者属性，可以选择性的使用override
		
	*）特质：trait
		1）相当于java的接口，实际上比接口功能强大
		2）与接口不同的是，特质可以定义属性和方法的实现
		3）一般情况下scala的类只能被继承单一的父类，但是如果是特质的话可以实现多继承，从结果上来看实现了多继承
		4）特质定义的方式与类类似，但是它使用的关键字是trait
		
	
	*）多态
		1）什么是多态：目的是让代码更加简洁，降低耦合
			有继承或者实现特质（接口）
			父类引用指向子类对象或者接口指向实现类
			方法需要重写
			
	
	*）模式匹配
		参考源码
	
	*）样例类	
		1）简单的说，在calss前面加个case就是一个样例类
			样例类有什么好处
			*）可以做模式匹配
			*）不用new关键字，即可创建实例 
			*）默认序列化
		
		
五：scala的高级内容：泛型，约束了类的范围
	*）泛型类
		泛型类（类声明时类名后面括号中即为类型参数），顾名思义，就是在类的声明中，定义一些泛型的类型，然后再field、method，就可以直接使用这些泛型类型
		使用泛型类，通常需要对类中的某些成员，比如某些field、method的参数或者变量，进行统一的类型限制，这样可以保证程序有更好的健壮性和稳定性
		如果说步使用泛型统一的类型限制，那么再后面的程序运行中，容易出现问题，比如说传入了不希望出现的类型，导致程序崩溃
		
		
	*）泛型函数
		泛型函数（方法声明时芳名后面括号中的类型参数），与泛型类类似，可以给某个函数再声明时指定泛型类型，然后再函数体中，多个变量或者返回值
		引用反射包	import scala.reflect.ClassTag
		ClassTag：表示scala在运行时的状态信息，这里表示调用时的数据类型
		
	*）泛型的上界和下界
		核心的概念：泛型的取值范围
		1：以普通的数据类型为例
			10<i <100;->规定了i的取值范围（10-100）
		2：泛型的取值范围：上界、下界
			定义几个类：（具有继承关系） Class A->Class B->Class C-> Class D
			定义泛型：
				D < T < B		->泛型T的取值范围是：B、C、D
		3：概念
			上界：定义  s <: T	这是类型上界的定义，也就是说s必须是类型T的子类（或者本身，自己也可以认为自己是自己的子类）
				UpperBound：类似于java中的T extends Comparable<T>
				
			下界：定义  s >: T	这是类型下界的定义，也就是说s必须是类型T的父类（或者本身，自己也可以认为自己是自己的父类）
				LowerBound: 类似于Java中的T super Comparable<T>
				
		4：举例
			参考源码
			
	*）隐式转换值
		优先级：传入>上下文类型一致的隐式值>默认值
	
	*）隐式转换函数
		
		
	*）视图界定：核心就是扩展了上界和下界的范围
		
	*）隐式转换函数
	*）斜变和逆变
	*）隐式参数
	*）隐式类
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	