大数据框架（处理海量离线数据/处理实时流式数据）
	一：以hadoop2.x版本为体系的海量数据处理框架
		离线数据分析，往往分析的式N+1的数据
		- Mapreduce
			并行计算，分而治之
			- HDFS（分布式数据的存储）
			- Yarn（分布式资源管理和任务调度）
			缺点：
				主要依赖磁盘，对磁盘依赖太高（io）
				shuflle，map讲数据写入到本次磁盘，reduce通过网络的方式将map task任务产生的数据拉取到reduce端
							
		- Hive 数据仓库的工具（从事数据挖掘方向，一定要掌握hive，hive的优化、调优）
			底层还是调用mr
			impala（性能非常高，基于内存的）
		
		- Sqoop
			桥梁：RDBMS（关系型数据库）-》HDFS/Hive
				  HDFS/Hive	-> RDBMS（关系型数据库)
		
		- HABSE	
			列式Nosql存储数据库，大数据的分布式数据库
		
		- Hadoop如果做机器学习的话需要使用mahout库
		
	二：以storm为体系的实时流式处理框架
		JStorm（Java编写，阿里巴巴）
		实时数据分析-》进行实时分析
		场景：股票交易、金融、电商、导航系统
		
	
	三：以spark为体系的大数据框架
		基于内存的
			将数据的中间结果写入到内存（内存+磁盘）
		
		核心编程：
			Spark Core：RDD（弹性分布式数据集），类似于Mr
			Spark Sql：类似于Hive
			Spark Streaming: Strom
				结构化流
			
		高级编程：
			机器学习、深度学习、人工智能
			Spark MLLib: 机器学习（推荐系统）
			Spark Graphx: 图计算（广告系统）
			
	四：Flink
		社区不够活跃，阿里已经放弃了JStorm，转而使用Flink基础开发的BLink
		阿里、美团、滴滴
		

Spark 学习计划
第一部分：scala编程语言
第二部分：Spark Core（最重要的内容）-》核心概念RDD
第三部分：Spark Sql（相当于Hive， 支持两种操作语句：SQL和DSL）-》底层依赖的还是RDD
第四部分：Spark Streaming：主要做实时数据分析，相当于Storm
	注意：Spark Streaming实时性比较差


学习建议：
	- 类比法
		Scala：Java
		Spark Core：Mr
		Spark Sql：Hive
		Spark Streaming：Core，基于CoreApi的高度封装
	
	- 听、练、记、理解
		- 听：认真听
		- 练：多练，自己敲代码，严禁拷贝
		- 记：做笔记
		- 思：思考、理解
		
======================================第一部分：Scala编程语言=============================================================================
一：scala语言基础
	*）scala简介
		1）多范式编程语言
			面向对象编程+面向函数式编程
			架构师阅读你的源码（review）
		2）编程简单
			代码量大概式10：1的关系
		3）scala的语言诞生了两个大数据框架：
			- spark
				分布式海量数据处理框架
			- kafka
				分布式流平台，基于消息的发布和订阅框架，存储数据
			
		4）学习网站
			官网：
				https://www.scala-lang.org/
			推特：
				http://twitter.github.io/scala_school/zh_cn/
				
		5）版本选择
			2.11.x版本， 推荐2.11.12
			
			备注说明：
				- spark 1.6版本			推荐的是scala 2.10.x版本
				- spark 2.x版本			推荐的scala 2.11.x版本
				
			
	*）scala的安装
		参考讲义
		
	
	*）写个scala程序
	
	*）常用的开发工具
		参考讲义
	
	*）变量的声明和定义
		有两种方式声明变量
		var（vriable） 和 val（value）
		var：可变的
		val：不可变的
		
		标准格式：val name:String = "tom"
			val：变量声明
			name：变量名
			：分隔符
			String：变量类型
			“tom”: 变量值
			
		不标准格式：
			 val name = "tom"
			 
			scala> val name:String = _
			<console>:11: error: unbound placeholder parameter
				   val name:String = _
									 ^
			scala> var name:String = _
			name: String = null

		注意：如果说变量的值是默认值的话，那么变量的声明一定是需要可变的，否则就没有声明变量的实际意义了
		
		String类型的默认值是null， int类型的默认值是0， Double的默认值是0.0
		
		一定注意：在Spark中能使用val的话，绝不使用var
		如果不显式的指定变量的类型，那么会根据变量的值进行自动的类型推倒
		
	*）scala数据类型
		常用的数据类型
			Byte：8位的有符号的		-128-127
			Char：
			Short：16位有符号		-32768-32767
			Int：  32位有符号	
			Long： 64位有符号	
			Float：浮点数
			Double：双精度
			Bolean
			
		跟java的区别
			在scala中，任何数据都是对象
				举例：数字 1 也是对象，因为它有方法
				
		在scala中存在很多增强数据类型的类，增加了对象的方法
	
		在scala中不存在自增、自减、三目运算符

	*）字符串插值操作
		scala> var src = "hello ${name}"
		src: String = hello ${name}

		scala> var name = "tom"
		name: String = tom

		scala> var src = s"hello ${name}"
		src: String = hello tom

	*）	Nothing表示在程序运行时产生的Exception
			scala> def f = throw new Exception("error")
			f: Nothing
			
		Null 、 Nil 、 Nothing 、 None 分别代表什么意思
		None 是一个object，是Option的子类型
			case object None extends Option[Nothing] {
			  def isEmpty = true
			  def get = throw new NoSuchElementException("None.get")
			}
			scala推荐在可能返回空的方法使用Option[X]作为返回类型。如果有值就返回Some[x](Some也是Option的子类)，否则返回None
			def get(key: A): Option[B] = {
				if (contains(key))
					Some(getValue(key))
				else
					None
			}
		Null 是所有AnyRef的子类，在scala的类型系统中，AnyRef是Any的子类，同时Any子类的还有AnyVal。对应java值类型的所有类型都是AnyVal的子类。所以Null可以赋值给所有的引用类型(AnyRef)，不能赋值给值类型，这个java的语义是相同的。 null是Null的唯一对象。
		
		Nothing 是所有类型的子类，也是Null的子类。Nothing没有对象，但是可以用来定义类型。例如，如果一个方法抛出异常，则异常的返回值类型就是Nothing(虽然不会返回) 
		
		Nil 是一个空的List，定义为List[Nothing]，根据List的定义List[+A]，所有Nil是所有List[T]的子类。
	
	*）scala的条件表达式
		有三种表达式的结构
		
		IF(boolean)  {}
		
		IF(boolean) {} else {}
		
		IF(i=1) {} else IF (i==2) {} else {}
	
		最大的区别：可以有返回值，并且可以用变量接收
		这时候返回的数据类型是Any，Any是所有数据类型的基类
		
	*）循环
		For 循环
			循环表达式
			在java中循环
			for(int i=0; i<=10;i++)
			
		While
		
		Do	。。。	While
		
		foeach
	
		需要注意的是：不能使用break跳出循环
		
	
二：scala面向函数式编程（最有特色的一部分）
	*）如何定义一个方法
		标准的方法定义格式：
			def m(x:Int, y:Int):Unit = x*y
			
			def：定义方法使用def作为关键字
			m: 方法名
			(x:Int, y:Int)：参数列表
			Unit：返回值类型，Unit表示无返回值，类似于Java的void
			x*y：方法体
			
			指定返回值是Int类型
			def m(x:Int, y:Int):Int = x*y
			
			忽略返回值类型，根据方法体的最后一句话进行自动的推导
			def m(x:Int, y:Int) = x*y
			
			如果方法中没有参数的话。那么小括号是可以省略的，但是调用的时候，不能加括号了
			def m = println("hello world!")
			
		
	*）如何定义一个函数	
		scala> val f = (x:Int, y:Int) => x*y
		f: (Int, Int) => Int = <function2>

		val：定义函数关键字
		f：函数名
		(Int, Int)：参数类型
		Int：返回值类型
		<function2>：函数参数的个数，最多只能由22个参数，如果想使用更多参数的话，使用变长参数
		
		匿名函数，不指明函数名称
		(x:Int, y:Int) => x*y
		
		无参函数
		scala> () => 2
		res12: () => Int = <function0>

		//调用函数的时候一定要加(), 不能省略，省略的话，会打印函数签名
		scala> val f2:(Int, Int) => Int = (x, y) => x*y
		f2: (Int, Int) => Int = <function2>
		f2：函数名称
		(Int, Int)：函数参数类型
		Int：返回值类型
		(x, y)：参数列表
		x*y：函数体
		
		//完整写法
		scala> val f3:(Int, Int) => Int = (x:Int, y:Int) => x*y 
		f3: (Int, Int) => Int = <function2>
		
	*）方法和函数的区别
		将方法转换成函数，采用神奇的下划线
		scala> def m(x:Int, y:Int) = x*y
		m: (x: Int, y: Int)Int

		scala> val m2 = m(2, 3)
		m2: Int = 6
		
		scala> m2 _					//下划线前面一定要加个空格
		res17: () => Int = <function0>

		
	*）函数的求值策略
		函数有两种求值策略
		1）call by value：
			对函数的实参求值， 且只求一次
			
		2）call by name：
			函数的实参在函数体内部每次用到的时候，都会被求值
		
	 
	*）高阶函数
		
	*）闭包
		就是函数的嵌套，即：在一个函数定义中，包含另外一个函数的定义；并且在内函数中可以访问外函数中的变量。
		

三：scala的集合
	*）数组
	*）集合
	*）集合的常用方法
	*）模式匹配
	*）样例类

	
四：scala面向对象编程
	*）面向对象的基本特征
	*）作用域
	*）伴生对象
	*）构造器
	*）scala中的object
	*）scala中的apply方法
	*）继承
	*）特质：trait
	*）多态
	
五：scala的高级内容：泛型
	*）泛型类
	*）泛型函数
	*）泛型的上界和下界
	*）视图界定：核心就是扩展了上界和下界的范围
	*）隐式转换函数
	*）斜变和逆变
	*）隐式参数
	*）隐式类
	*）隐式转换
	
	